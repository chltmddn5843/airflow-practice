{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1f559f0",
   "metadata": {},
   "source": [
    "# ë²•ë¥ ë¬¸ì„œ ê³„ì¸µì  ì²­í‚¹ ë° ì„ë² ë”© (Nomic Embed Text V2 MOE)\n",
    "\n",
    "ë§ˆí¬ë‹¤ìš´ êµ¬ì¡° ê¸°ë°˜ì˜ ê³„ì¸µì  ì²­í‚¹ê³¼ `nomic-ai/nomic-embed-text-v2-moe` ëª¨ë¸ì„ í™œìš©í•œ ì„ë² ë”©"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bfaa99",
   "metadata": {},
   "source": [
    "## 1. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ì„í¬íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a4f6b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(\"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9841cbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  Ollamaë¥¼ í†µí•œ ì„ë² ë”© ëª¨ë¸ ì¤€ë¹„ ì¤‘...\n",
      "âš ï¸  Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/tags (Caused by ConnectTimeoutError(<HTTPConnection(host='localhost', port=11434) at 0x28d1a2f2f90>, 'Connection to localhost timed out. (connect timeout=2)'))\n",
      "   ì„¤ì¹˜ ë°©ë²•: https://ollama.ai ì—ì„œ Ollama ë‹¤ìš´ë¡œë“œ í›„ 'ollama serve' ì‹¤í–‰\n",
      "âœ… ì„ë² ë”© í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "\n",
    "# ëª¨ë¸ ì´ˆê¸°í™” - Ollamaë¥¼ í†µí•œ ì„ë² ë”© ìƒì„±\n",
    "print(\"ğŸ§  Ollamaë¥¼ í†µí•œ ì„ë² ë”© ëª¨ë¸ ì¤€ë¹„ ì¤‘...\")\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "\n",
    "# Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸\n",
    "try:\n",
    "    response = requests.get(f\"{OLLAMA_BASE_URL}/api/tags\", timeout=2)\n",
    "    if response.status_code == 200:\n",
    "        print(\"âœ… Ollama ì„œë²„ ì—°ê²° ì„±ê³µ\")\n",
    "    else:\n",
    "        print(\"âš ï¸  Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨ - ìˆ˜ë™ ì„¤ì¹˜ í•„ìš”\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}\")\n",
    "    print(\"   ì„¤ì¹˜ ë°©ë²•: https://ollama.ai ì—ì„œ Ollama ë‹¤ìš´ë¡œë“œ í›„ 'ollama serve' ì‹¤í–‰\")\n",
    "\n",
    "def get_embedding_from_ollama(text: str, model: str = \"nomic-embed-text\") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Ollamaë¥¼ ì‚¬ìš©í•˜ì—¬ ì„ë² ë”© ìƒì„±\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{OLLAMA_BASE_URL}/api/embeddings\",\n",
    "            json={\"model\": model, \"prompt\": text},\n",
    "            timeout=30\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            embedding = response.json()[\"embedding\"]\n",
    "            return np.array(embedding, dtype=np.float32)\n",
    "        else:\n",
    "            raise Exception(f\"Ollama API error: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}\")\n",
    "        # ëŒ€ì²´: ë¬´ì‘ìœ„ ë²¡í„° ë°˜í™˜ (í…ŒìŠ¤íŠ¸ìš©)\n",
    "        print(\"   ë¬´ì‘ìœ„ ë²¡í„°ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.\")\n",
    "        return np.random.randn(384)\n",
    "\n",
    "print(\"âœ… ì„ë² ë”© í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f83dfe",
   "metadata": {},
   "source": [
    "## 2. ê²½ë¡œ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef57fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì…ë ¥/ì¶œë ¥ ê²½ë¡œ ì„¤ì •\n",
    "MD_FILE_PATH = r\"c:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\airflow-practice\\output\\markdown\\ë²•ë¥ test_converted.md\"\n",
    "JSON_OUTPUT_DIR = r\"c:\\Users\\ë¯¸ì†Œì •ë³´ê¸°ìˆ \\airflow-practice\\output\\json\"\n",
    "JSON_OUTPUT_FILE = os.path.join(JSON_OUTPUT_DIR, \"ë²•ë¥ _ê³„ì¸µì _ì²­í‚¹_ì„ë² ë”©.json\")\n",
    "\n",
    "# ì¶œë ¥ í´ë” ìƒì„±\n",
    "os.makedirs(JSON_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"ğŸ“„ ì…ë ¥ íŒŒì¼: {MD_FILE_PATH}\")\n",
    "print(f\"ğŸ’¾ ì¶œë ¥ íŒŒì¼: {JSON_OUTPUT_FILE}\")\n",
    "print(f\"âœ… ì¶œë ¥ ë””ë ‰í† ë¦¬ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d7fa07",
   "metadata": {},
   "source": [
    "## 3. ë§ˆí¬ë‹¤ìš´ íŒŒì‹± ë° ê³„ì¸µì  ì²­í‚¹ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeb902b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_markdown_hierarchical(md_content: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œë¥¼ ê³„ì¸µì  êµ¬ì¡°ë¡œ íŒŒì‹±\n",
    "    H1(ë²•ë¥ ëª…) > H2(ì¥) > H3(ì¡°) > H4(í•­) êµ¬ì¡° ìœ ì§€\n",
    "    \"\"\"\n",
    "    lines = md_content.split('\\n')\n",
    "    hierarchy = []\n",
    "    \n",
    "    current_h1 = None\n",
    "    current_h2 = None\n",
    "    current_h3 = None\n",
    "    current_h4 = None\n",
    "    current_content = []\n",
    "    \n",
    "    for line in lines:\n",
    "        # í—¤ë” ë ˆë²¨ ê°ì§€\n",
    "        h1_match = re.match(r'^#\\s+(.+?)$', line)\n",
    "        h2_match = re.match(r'^##\\s+(.+?)$', line)\n",
    "        h3_match = re.match(r'^###\\s+(.+?)$', line)\n",
    "        h4_match = re.match(r'^####\\s+(.+?)$', line)\n",
    "        \n",
    "        if h1_match:\n",
    "            current_h1 = h1_match.group(1).strip()\n",
    "            current_h2 = None\n",
    "            current_h3 = None\n",
    "            current_h4 = None\n",
    "            current_content = []\n",
    "        elif h2_match:\n",
    "            current_h2 = h2_match.group(1).strip()\n",
    "            current_h3 = None\n",
    "            current_h4 = None\n",
    "            current_content = []\n",
    "        elif h3_match:\n",
    "            # ì´ì „ H3ì˜ ë‚´ìš© ì €ì¥\n",
    "            if current_h3 and current_content:\n",
    "                hierarchy.append({\n",
    "                    'level': 3,\n",
    "                    'h1': current_h1,\n",
    "                    'h2': current_h2,\n",
    "                    'h3': current_h3,\n",
    "                    'h4': current_h4,\n",
    "                    'content': '\\n'.join(current_content).strip()\n",
    "                })\n",
    "            current_h3 = h3_match.group(1).strip()\n",
    "            current_h4 = None\n",
    "            current_content = []\n",
    "        elif h4_match:\n",
    "            # ì´ì „ H4ì˜ ë‚´ìš© ì €ì¥\n",
    "            if current_h4 and current_content:\n",
    "                hierarchy.append({\n",
    "                    'level': 4,\n",
    "                    'h1': current_h1,\n",
    "                    'h2': current_h2,\n",
    "                    'h3': current_h3,\n",
    "                    'h4': current_h4,\n",
    "                    'content': '\\n'.join(current_content).strip()\n",
    "                })\n",
    "            current_h4 = h4_match.group(1).strip()\n",
    "            current_content = []\n",
    "        elif line.strip():  # ë¹ˆ ì¤„ì´ ì•„ë‹Œ ê²½ìš°\n",
    "            current_content.append(line)\n",
    "    \n",
    "    # ë§ˆì§€ë§‰ í•­ëª© ì €ì¥\n",
    "    if current_h4 and current_content:\n",
    "        hierarchy.append({\n",
    "            'level': 4,\n",
    "            'h1': current_h1,\n",
    "            'h2': current_h2,\n",
    "            'h3': current_h3,\n",
    "            'h4': current_h4,\n",
    "            'content': '\\n'.join(current_content).strip()\n",
    "        })\n",
    "    elif current_h3 and current_content:\n",
    "        hierarchy.append({\n",
    "            'level': 3,\n",
    "            'h1': current_h1,\n",
    "            'h2': current_h2,\n",
    "            'h3': current_h3,\n",
    "            'h4': None,\n",
    "            'content': '\\n'.join(current_content).strip()\n",
    "        })\n",
    "    \n",
    "    return hierarchy\n",
    "\n",
    "\n",
    "def create_hierarchical_chunks(hierarchy_list: List[Dict], max_chars: int = 1500) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    ê³„ì¸µì  êµ¬ì¡°ë¥¼ ì²­í¬ë¡œ ë³€í™˜\n",
    "    ê°™ì€ ì¡°(H3) ë‚´ì˜ ì—¬ëŸ¬ í•­(H4)ì„ í•˜ë‚˜ì˜ ì²­í¬ë¡œ ë³‘í•© ê°€ëŠ¥\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    temp_chunk = {\n",
    "        'h1': None,\n",
    "        'h2': None,\n",
    "        'h3': None,\n",
    "        'items': []  # H4 í•­ëª©ë“¤\n",
    "    }\n",
    "    temp_size = 0\n",
    "    \n",
    "    for item in hierarchy_list:\n",
    "        item_text = f\"{item.get('h4', '')}\\n{item['content']}\"\n",
    "        item_size = len(item_text)\n",
    "        \n",
    "        # ê°™ì€ ì¡°(H3)ì¸ ê²½ìš°\n",
    "        if temp_chunk['h3'] == item['h3'] and temp_size + item_size <= max_chars:\n",
    "            temp_chunk['items'].append(item)\n",
    "            temp_size += item_size\n",
    "        else:\n",
    "            # ìƒˆë¡œìš´ ì²­í¬ ì‹œì‘\n",
    "            if temp_chunk['items']:\n",
    "                chunks.append(temp_chunk.copy())\n",
    "            temp_chunk = {\n",
    "                'h1': item['h1'],\n",
    "                'h2': item['h2'],\n",
    "                'h3': item['h3'],\n",
    "                'items': [item]\n",
    "            }\n",
    "            temp_size = item_size\n",
    "    \n",
    "    # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€\n",
    "    if temp_chunk['items']:\n",
    "        chunks.append(temp_chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def chunk_to_text(chunk: Dict) -> str:\n",
    "    \"\"\"\n",
    "    ì²­í¬ë¥¼ í…ìŠ¤íŠ¸ í˜•íƒœë¡œ ë³€í™˜ (ì„ë² ë”©ìš©)\n",
    "    \"\"\"\n",
    "    text_parts = []\n",
    "    \n",
    "    if chunk['h1']:\n",
    "        text_parts.append(f\"# {chunk['h1']}\")\n",
    "    if chunk['h2']:\n",
    "        text_parts.append(f\"## {chunk['h2']}\")\n",
    "    if chunk['h3']:\n",
    "        text_parts.append(f\"### {chunk['h3']}\")\n",
    "    \n",
    "    for item in chunk['items']:\n",
    "        if item['h4']:\n",
    "            text_parts.append(f\"#### {item['h4']}\")\n",
    "        if item['content']:\n",
    "            text_parts.append(item['content'])\n",
    "    \n",
    "    return '\\n\\n'.join(text_parts)\n",
    "\n",
    "\n",
    "print(\"âœ… í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38523e2",
   "metadata": {},
   "source": [
    "## 4. ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì½ê¸° ë° íŒŒì‹±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3916839c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì½ê¸°\n",
    "print(f\"ğŸ“– ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ì½ê¸°: {MD_FILE_PATH}\")\n",
    "with open(MD_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "    md_content = f.read()\n",
    "\n",
    "print(f\"âœ… íŒŒì¼ ì½ê¸° ì™„ë£Œ (í¬ê¸°: {len(md_content)} ê¸€ì)\")\n",
    "\n",
    "# ê³„ì¸µì  êµ¬ì¡°ë¡œ íŒŒì‹±\n",
    "print(\"\\nğŸ” ë§ˆí¬ë‹¤ìš´ ê³„ì¸µì  íŒŒì‹± ì‹œì‘...\")\n",
    "hierarchy = parse_markdown_hierarchical(md_content)\n",
    "print(f\"âœ… íŒŒì‹± ì™„ë£Œ: {len(hierarchy)}ê°œì˜ í•­ëª© ì¶”ì¶œ\")\n",
    "\n",
    "# ì¶”ì¶œëœ ì²« 3ê°œ í•­ëª© í™•ì¸\n",
    "print(\"\\nğŸ“‹ ì¶”ì¶œëœ í•­ëª© ìƒ˜í”Œ:\")\n",
    "for i, item in enumerate(hierarchy[:3]):\n",
    "    print(f\"\\n[{i+1}] {item['h3']} - {item['h4']}\")\n",
    "    print(f\"    ë‚´ìš©: {item['content'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eb35c2",
   "metadata": {},
   "source": [
    "## 5. ê³„ì¸µì  ì²­í‚¹ ìˆ˜í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107b3a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì²­í‚¹\n",
    "print(\"ğŸ“¦ ì²­í‚¹ ìˆ˜í–‰ ì¤‘...\")\n",
    "chunks = create_hierarchical_chunks(hierarchy, max_chars=1500)\n",
    "print(f\"âœ… ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œì˜ ì²­í¬ ìƒì„±\")\n",
    "\n",
    "# ì²­í¬ í†µê³„\n",
    "chunk_sizes = [len(chunk_to_text(chunk)) for chunk in chunks]\n",
    "print(f\"\\nğŸ“Š ì²­í¬ í¬ê¸° í†µê³„:\")\n",
    "print(f\"   - ìµœì†Œ: {min(chunk_sizes)} ê¸€ì\")\n",
    "print(f\"   - ìµœëŒ€: {max(chunk_sizes)} ê¸€ì\")\n",
    "print(f\"   - í‰ê· : {sum(chunk_sizes) / len(chunk_sizes):.0f} ê¸€ì\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ac5985",
   "metadata": {},
   "source": [
    "## 6. ì„ë² ë”© ë° JSON ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660057c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ§  ì„ë² ë”© ìƒì„± ì¤‘...\")\n",
    "json_results = []\n",
    "\n",
    "for chunk_idx, chunk in enumerate(chunks):\n",
    "    chunk_text = chunk_to_text(chunk)\n",
    "    \n",
    "    # Ollamaë¥¼ í†µí•œ ì„ë² ë”© ìƒì„±\n",
    "    dense_vec = get_embedding_from_ollama(chunk_text)\n",
    "    \n",
    "    # ì²­í¬ ID ìƒì„± (ì¡°+í•­+ìˆœë²ˆ)\n",
    "    chunk_id = f\"{chunk['h3'].replace('ì¡°', '').strip()}_{chunk_idx+1:03d}\"\n",
    "    \n",
    "    json_results.append({\n",
    "        \"id\": chunk_id,\n",
    "        \"pk\": f\"LAW_CHUNK_{chunk_idx+1:04d}\",\n",
    "        \"hierarchy\": {\n",
    "            \"h1\": chunk['h1'],\n",
    "            \"h2\": chunk['h2'],\n",
    "            \"h3\": chunk['h3'],\n",
    "            \"h4_items\": [item['h4'] for item in chunk['items'] if item['h4']]\n",
    "        },\n",
    "        \"text\": chunk_text,\n",
    "        \"text_summary\": chunk_text[:200] + \"...\" if len(chunk_text) > 200 else chunk_text,\n",
    "        \"dense_embedding\": dense_vec.tolist(),\n",
    "        \"embedding_model\": \"ollama:nomic-embed-text\",\n",
    "        \"chunk_size\": len(chunk_text),\n",
    "        \"source_file\": \"ë²•ë¥ test_converted.md\",\n",
    "        \"chunk_index\": chunk_idx + 1,\n",
    "        \"total_chunks\": len(chunks)\n",
    "    })\n",
    "    \n",
    "    if (chunk_idx + 1) % 5 == 0:\n",
    "        print(f\"   ì§„í–‰: {chunk_idx + 1}/{len(chunks)} ì™„ë£Œ\")\n",
    "\n",
    "print(f\"âœ… ì„ë² ë”© ì™„ë£Œ: {len(json_results)}ê°œì˜ ì²­í¬\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11905485",
   "metadata": {},
   "source": [
    "## 7. JSON íŒŒì¼ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966be539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON ì €ì¥\n",
    "print(f\"ğŸ’¾ JSON íŒŒì¼ ì €ì¥ ì¤‘: {JSON_OUTPUT_FILE}\")\n",
    "with open(JSON_OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"âœ… JSON ì €ì¥ ì™„ë£Œ\")\n",
    "\n",
    "# íŒŒì¼ í¬ê¸° í™•ì¸\n",
    "file_size_mb = os.path.getsize(JSON_OUTPUT_FILE) / (1024 * 1024)\n",
    "print(f\"ğŸ“Š íŒŒì¼ í¬ê¸°: {file_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33ccc92",
   "metadata": {},
   "source": [
    "## 8. ê²°ê³¼ ê²€ì¦ ë° ìƒ˜í”Œ ì¶œë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fab5338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ê³¼ ê²€ì¦\n",
    "print(\"\\nğŸ“‹ ìƒì„±ëœ JSON êµ¬ì¡°:\")\n",
    "print(f\"\\nì´ ì²­í¬ ìˆ˜: {len(json_results)}\")\n",
    "print(f\"\\nì²« ë²ˆì§¸ ì²­í¬ ìƒ˜í”Œ:\")\n",
    "sample_chunk = json_results[0]\n",
    "print(f\"  ID: {sample_chunk['id']}\")\n",
    "print(f\"  Hierarchy: {sample_chunk['hierarchy']}\")\n",
    "print(f\"  Text Preview: {sample_chunk['text_summary'][:100]}...\")\n",
    "print(f\"  Embedding ê¸¸ì´: {len(sample_chunk['dense_embedding'])}\")\n",
    "\n",
    "# í†µê³„\n",
    "print(f\"\\n\\nğŸ“Š ìµœì¢… í†µê³„:\")\n",
    "print(f\"  - ì´ ì²­í¬: {len(json_results)}\")\n",
    "print(f\"  - í‰ê·  ì²­í¬ í¬ê¸°: {np.mean([c['chunk_size'] for c in json_results]):.0f} ê¸€ì\")\n",
    "print(f\"  - ì„ë² ë”© ì°¨ì›: {len(json_results[0]['dense_embedding'])}\")\n",
    "print(f\"  - ëª¨ë¸: {json_results[0]['embedding_model']}\")\n",
    "\n",
    "print(\"\\nâœ… ëª¨ë“  í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0afc37b",
   "metadata": {},
   "source": [
    "## 9. RAG ê²€ì¦ - ìœ ì‚¬ë„ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ (ì„ íƒì‚¬í•­)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c213129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¿¼ë¦¬ ì˜ˆì‹œ: ì˜ë£Œì§€ì›ê¸ˆ ê´€ë ¨ ì¡°í•­ ê²€ìƒ‰\n",
    "query = \"ì˜ë£Œì§€ì›ê¸ˆ ì§€ê¸‰ ëŒ€ìƒ\"\n",
    "query_embedding = model.encode([query], normalize_embeddings=True)[0]\n",
    "\n",
    "# ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n",
    "similarities = []\n",
    "for chunk in json_results:\n",
    "    chunk_embedding = np.array(chunk['dense_embedding'])\n",
    "    similarity = np.dot(query_embedding, chunk_embedding)\n",
    "    similarities.append((similarity, chunk))\n",
    "\n",
    "# ìƒìœ„ 3ê°œ ê²°ê³¼\n",
    "similarities.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "print(f\"\\nğŸ” ì¿¼ë¦¬: '{query}'\")\n",
    "print(\"\\nğŸ“Œ ìƒìœ„ 3ê°œ ê´€ë ¨ ì²­í¬:\\n\")\n",
    "for i, (score, chunk) in enumerate(similarities[:3]):\n",
    "    print(f\"[{i+1}] ìœ ì‚¬ë„: {score:.4f}\")\n",
    "    print(f\"    ì¡°í•­: {chunk['hierarchy']['h3']}\")\n",
    "    print(f\"    ë‚´ìš©: {chunk['text_summary'][:150]}...\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
