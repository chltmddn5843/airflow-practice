"""
Legal ETL Pipeline for Korean Law & Court Precedent RAG System

ì´ DAGëŠ” êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„°ì—ì„œ íŒë¡€ XML ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì—¬
PostgreSQLì— ê³„ì¸µì  êµ¬ì¡°(ì¡°/í•­/ë¬¸)ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

Data Flow:
  Extract (í¬ë¡¤ë§) â†’ Transform (íŒŒì‹±) â†’ Load (DB ì €ì¥) â†’ Verify (ê²€ì¦)

Hierarchical Structure:
  Parent (ì¡°/Article): íŒì‹œì‚¬í•­ [1], [2] ë“±ì˜ ì£¼ìš” í•­ëª©
  Child (ë¬¸/Sentence): ê° ì¡°ë¥¼ 200ìì”© ë¶„í• í•œ ë¬¸ì¥ ë‹¨ìœ„
"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from datetime import datetime
import requests
from bs4 import BeautifulSoup
import re
import logging
import pandas as pd

logger = logging.getLogger(__name__)

# ============================================================================
# Configuration
# ============================================================================
LEGAL_API_BASE = 'https://www.law.go.kr/LSW/precInfoP.do?precSeq={id}&mode=0&vSct=*'
DEFAULT_CASE_IDS = ['64441']  # ì˜ˆì‹œìš© íŒë¡€ ID
CHUNK_SENTENCE_SIZE = 200     # ë¬¸ ë‹¨ìœ„ ì²­í¬ í¬ê¸°

# í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„° (ì‹¤ì œ API ëŒ€ì‹  ì‚¬ìš©)
SAMPLE_XML = """<?xml version="1.0" encoding="UTF-8"?>
<íŒë¡€>
    <íŒë¡€ì •ë³´ì¼ë ¨ë²ˆí˜¸>64441</íŒë¡€ì •ë³´ì¼ë ¨ë²ˆí˜¸>
    <ì‚¬ê±´ëª…>ì‚¬ê±´ëª… í…ŒìŠ¤íŠ¸</ì‚¬ê±´ëª…>
    <íŒë¡€ë‚´ìš©>
        <íŒì‹œì‚¬í•­>
            [1] ë¯¼ë²•ìƒ ë°°ìš°ìëŠ” ë²•ë¥ í˜¼ ë°°ìš°ìë¥¼ ì˜ë¯¸í•œë‹¤ëŠ” ì ì—ì„œ í˜¼ì¸ì´ ìœ íš¨í•˜ê²Œ ì„±ë¦½í•˜ì§€ ì•Šìœ¼ë©´ ë²•ë¥ ìƒ ë°°ìš°ìê°€ ì•„ë‹ˆë¼ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.
            [2] êµ¬ ë¯¼ë²• ì œ835ì¡° ì œ1í•­ì´ ì •í•œ ì¹œìƒìì˜ ìš”ê±´ì€ ì²«ì§¸, ë¶€ëª¨ê°€ í˜¼ì¸ ì¤‘ì— ìˆì–´ì•¼ í•˜ê³ , ë‘˜ì§¸, ìë…€ê°€ í˜¼ì¸ ì¤‘ì— íƒœì–´ë‚˜ì•¼ í•œë‹¤ëŠ” ê²ƒì´ë‹¤.
        </íŒì‹œì‚¬í•­>
    </íŒë¡€ë‚´ìš©>
</íŒë¡€>
"""

# ============================================================================
# Helper Functions
# ============================================================================

def parse_xml_to_master(soup):
    """XMLì—ì„œ íŒë¡€ ë§ˆìŠ¤í„° ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    # ë‹¤ì–‘í•œ íƒœê·¸ëª… ì‹œë„
    case_id_elem = (soup.find('íŒë¡€ì •ë³´ì¼ë ¨ë²ˆí˜¸') or 
                    soup.find('procSeq') or 
                    soup.find('id'))
    
    if not case_id_elem or not case_id_elem.text:
        logger.warning("âš  case_id not found - parsing HTML structure")
        # HTMLì—ì„œ ì¶”ì¶œ ì‹œë„
        case_id = 'UNKNOWN'
    else:
        case_id = case_id_elem.text.strip()
    
    title_elem = soup.find('ì‚¬ê±´ëª…') or soup.find('caseNm')
    title = title_elem.text.strip() if title_elem else "N/A"
    
    content_elem = soup.find('íŒë¡€ë‚´ìš©') or soup.find('prec')
    content = content_elem.get_text() if content_elem else ""
    
    logger.info(f"â†’ Parsed: case_id={case_id}, title={title[:50] if title else 'N/A'}...")
    
    if not content or len(content) < 10:
        logger.warning(f"âš  Minimal content found: {len(content)} bytes")
    
    return {
        'case_id': case_id if case_id != 'UNKNOWN' else f"case_{id(soup)}",
        'title': title,
        'full_text': content,
    } if content and len(content) > 10 else None


def split_into_chunks(article_text, parent_chunk_id, case_id):
    """ì¡° ë‹¨ìœ„ í…ìŠ¤íŠ¸ë¥¼ ì¡°/ë¬¸ ë‹¨ìœ„ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
    # ë¶€ëª¨ ì²­í¬ (ì¡°)
    yield (parent_chunk_id, case_id, 'ì¡°', article_text, case_id)
    
    # ìì‹ ì²­í¬ (ë¬¸)
    sentences = [
        article_text[i:i+CHUNK_SENTENCE_SIZE]
        for i in range(0, len(article_text), CHUNK_SENTENCE_SIZE)
    ]
    
    for sent_idx, sentence in enumerate(sentences):
        if sentence.strip():
            sentence_chunk_id = f"{parent_chunk_id}_ë¬¸_{sent_idx+1}"
            yield (sentence_chunk_id, case_id, 'ë¬¸', sentence.strip(), parent_chunk_id)

# ============================================================================
# Task Functions
# ============================================================================

def setup_database():
    """PostgreSQL í…Œì´ë¸”ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    pg_hook = PostgresHook(postgres_conn_id='postgres_default')
    connection = pg_hook.get_sqlalchemy_engine()
    
    with connection.begin() as conn:
        # Master table
        master_sql = """
        CREATE TABLE IF NOT EXISTS legal_master (
            case_id VARCHAR(50) PRIMARY KEY,
            title VARCHAR(500),
            full_text TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
        """
        
        # Chunks table
        chunks_sql = """
        CREATE TABLE IF NOT EXISTS legal_chunks (
            chunk_id VARCHAR(100) PRIMARY KEY,
            case_id VARCHAR(50) NOT NULL REFERENCES legal_master(case_id) ON DELETE CASCADE,
            level VARCHAR(20) NOT NULL,
            content TEXT NOT NULL,
            parent_id VARCHAR(100),
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
        CREATE INDEX IF NOT EXISTS idx_legal_chunks_case ON legal_chunks(case_id);
        CREATE INDEX IF NOT EXISTS idx_legal_chunks_level ON legal_chunks(level);
        """
        
        conn.execute(master_sql)
        conn.execute(chunks_sql)
        logger.info("âœ“ Tables created successfully")


def extract_legal_data(**kwargs):
    """êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„°ì—ì„œ íŒë¡€ XMLì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤."""
    raw_xml_list = []
    headers = {'User-Agent': 'Mozilla/5.0'}
    ti = kwargs['ti']
    
    logger.info(f"â³ Starting extraction for {len(DEFAULT_CASE_IDS)} case IDs...")
    
    for case_id in DEFAULT_CASE_IDS:
        try:
            url = LEGAL_API_BASE.format(id=case_id)
            logger.info(f"â†’ Fetching: {url}")
            response = requests.get(url, headers=headers, timeout=10)
            
            if response.status_code == 200:
                response.encoding = 'utf-8'
                content = response.text
                raw_xml_list.append(content)
                logger.info(f"âœ“ Extracted: {case_id} ({len(content)} bytes)")
            else:
                logger.warning(f"âœ— Failed {case_id}: HTTP {response.status_code}")
        except Exception as e:
            logger.error(f"âœ— Error extracting {case_id}: {type(e).__name__}: {str(e)}")
    
    # í…ŒìŠ¤íŠ¸ìš©: ì‹¤ì œ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìƒ˜í”Œ ì‚¬ìš©
    if not raw_xml_list:
        logger.warning("âš  No data extracted from API! Using SAMPLE_XML for testing...")
        raw_xml_list = [SAMPLE_XML]
    
    # ëª¨ë“  ê²½ìš°ì— XComì— ì €ì¥
    logger.info(f"ğŸ“Š Pushing {len(raw_xml_list)} documents to XCom...")
    ti.xcom_push(key='xml_data', value=raw_xml_list)
    logger.info(f"âœ“ XCom push successful ({len(raw_xml_list)} items)")
    return len(raw_xml_list)


def transform_and_load(**kwargs):
    """XMLì„ íŒŒì‹±í•˜ì—¬ PostgreSQLì— ì €ì¥í•©ë‹ˆë‹¤."""
    pg_hook = PostgresHook(postgres_conn_id='postgres_default')
    ti = kwargs['ti']
    raw_xmls = ti.xcom_pull(task_ids='extract', key='xml_data')
    
    logger.info(f"ğŸ“¥ XCom pull result: {type(raw_xmls)} - {len(raw_xmls) if raw_xmls else 0} items")

    if not raw_xmls:
        logger.warning("âš  No data to process! XCom pull returned None or empty list.")
        return

    logger.info(f"â³ Processing {len(raw_xmls)} documents...")
    connection = pg_hook.get_sqlalchemy_engine()
    inserted_masters = 0
    inserted_chunks = 0

    for idx, xml_content in enumerate(raw_xmls):
        try:
            if not xml_content or len(xml_content) < 10:
                logger.warning(f"âš  Document {idx}: Empty or too short ({len(xml_content)} bytes)")
                continue
                
            soup = BeautifulSoup(xml_content, 'lxml-xml')
            master_info = parse_xml_to_master(soup)
            
            if not master_info:
                logger.warning(f"âš  Document {idx}: Missing case_id or parsing failed")
                continue
            
            case_id = master_info['case_id']
            logger.info(f"â†’ Processing case: {case_id}")
            
            with connection.begin() as conn:
                # Master table insert
                master_sql = """
                INSERT INTO legal_master (case_id, title, full_text)
                VALUES (%s, %s, %s)
                ON CONFLICT (case_id) DO UPDATE SET title = EXCLUDED.title;
                """
                conn.execute(master_sql, (case_id, master_info['title'], master_info['full_text']))
                inserted_masters += 1
                logger.info(f"âœ“ Inserted master: {case_id}")

                # Chunks table insert
                precepts_elem = soup.find('íŒì‹œì‚¬í•­')
                if precepts_elem:
                    precepts_text = precepts_elem.get_text()
                    articles = re.findall(r'(\[\d+\].+?)(?=\[\d+\]|$)', precepts_text, re.DOTALL)
                    
                    chunk_sql = """
                    INSERT INTO legal_chunks (chunk_id, case_id, level, content, parent_id)
                    VALUES (%s, %s, %s, %s, %s)
                    ON CONFLICT (chunk_id) DO UPDATE SET content = EXCLUDED.content;
                    """
                    
                    for art_idx, article in enumerate(articles):
                        article_clean = article.strip()
                        if not article_clean:
                            continue
                        
                        parent_chunk_id = f"{case_id}_ì¡°_{art_idx+1}"
                        
                        for chunk_data in split_into_chunks(article_clean, parent_chunk_id, case_id):
                            conn.execute(chunk_sql, chunk_data)
                            inserted_chunks += 1
                        
                        logger.info(f"âœ“ Inserted chunks for article {art_idx+1}")
                else:
                    logger.warning(f"âš  No íŒì‹œì‚¬í•­ found in case {case_id}")
                    
        except Exception as e:
            logger.error(f"âœ— Error processing document {idx}: {type(e).__name__}: {str(e)}")
            continue
    
    logger.info(f"ğŸ“Š Summary: {inserted_masters} masters, {inserted_chunks} chunks inserted")


def verify_data():
    """DB ì €ì¥ ê²°ê³¼ë¥¼ ê²€ì¦í•©ë‹ˆë‹¤."""
    pg_hook = PostgresHook(postgres_conn_id='postgres_default')
    connection = pg_hook.get_sqlalchemy_engine()
    
    with connection.begin() as conn:
        master_count = pd.read_sql("SELECT COUNT(*) as cnt FROM legal_master;", conn)
        chunks_count = pd.read_sql("SELECT COUNT(*) as cnt FROM legal_chunks;", conn)
        level_dist = pd.read_sql("SELECT level, COUNT(*) as cnt FROM legal_chunks GROUP BY level ORDER BY level;", conn)
        
        logger.info(f"âœ“ Master records: {master_count['cnt'].values[0]}")
        logger.info(f"âœ“ Total chunks: {chunks_count['cnt'].values[0]}")
        logger.info(f"âœ“ Level distribution:\n{level_dist.to_string(index=False)}")

# ============================================================================
# DAG Definition
# ============================================================================

with DAG(
    dag_id='legal_etl_pipeline_v1',
    description='Legal-Link: ë²•ë ¹/íŒë¡€ RAG ì‹œìŠ¤í…œ ETL íŒŒì´í”„ë¼ì¸',
    start_date=datetime(2025, 1, 1),
    schedule_interval=None,
    catchup=False,
    tags=['legal', 'rag', 'etl'],
) as dag:

    setup_task = PythonOperator(
        task_id='setup',
        python_callable=setup_database,
        retries=1,
        doc="PostgreSQL í…Œì´ë¸” ì´ˆê¸°í™”",
    )
    
    extract_task = PythonOperator(
        task_id='extract',
        python_callable=extract_legal_data,
        retries=2,
        doc="êµ­ê°€ë²•ë ¹ì •ë³´ì„¼í„°ì—ì„œ íŒë¡€ XML í¬ë¡¤ë§",
    )

    load_task = PythonOperator(
        task_id='load',
        python_callable=transform_and_load,
        retries=1,
        doc="XML íŒŒì‹± ë° PostgreSQL ì €ì¥",
    )

    verify_task = PythonOperator(
        task_id='verify',
        python_callable=verify_data,
        doc="ë°ì´í„° ê²€ì¦ ë° í†µê³„",
    )

    setup_task >> extract_task >> load_task >> verify_task
